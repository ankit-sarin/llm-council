"""
LLM Council - Gradio Interface

Multi-model debate system with peer review and chairman synthesis.
"""

import asyncio
import gradio as gr

from config import (
    AVAILABLE_MODELS,
    CHAIRMAN_MODEL,
    DEFAULT_ENABLED_MODELS,
    MODEL_DISPLAY_NAMES,
    get_display_name,
)
from council import (
    CouncilResult,
    ConsensusScore,
    ModelResponse,
    PeerReview,
    get_initial_responses,
    get_peer_reviews,
    get_chairman_synthesis,
    stream_initial_responses,
    stream_peer_reviews,
    stream_initial_responses_live,
    stream_peer_reviews_live,
    calculate_consensus,
)

import json
import os
from datetime import datetime
from pathlib import Path

# --- Session Storage ---

SESSIONS_DIR = Path(__file__).parent / "sessions"
SESSIONS_DIR.mkdir(exist_ok=True)


def save_session(
    question: str,
    responses: dict[str, "ModelResponse"],
    reviews: dict[str, "PeerReview"],
    synthesis: str,
    consensus: ConsensusScore,
    models: list[str]
) -> dict:
    """Save a council session to JSON file."""
    timestamp = datetime.now()
    session_id = timestamp.strftime("%Y%m%d_%H%M%S")

    session_data = {
        "id": session_id,
        "timestamp": timestamp.isoformat(),
        "question": question,
        "models": [get_display_name(m) for m in models],
        "responses": {
            get_display_name(model): {
                "content": resp.content,
                "elapsed_seconds": resp.elapsed_seconds
            }
            for model, resp in responses.items()
        },
        "reviews": {
            get_display_name(reviewer): {
                "rankings": review.rankings,
                "analysis": review.analysis
            }
            for reviewer, review in reviews.items()
        },
        "consensus": {
            "score": consensus.score,
            "level": consensus.level,
            "description": consensus.description,
            "disagreement_areas": consensus.disagreement_areas
        },
        "synthesis": synthesis
    }

    filepath = SESSIONS_DIR / f"session_{session_id}.json"
    with open(filepath, "w") as f:
        json.dump(session_data, f, indent=2)

    return session_data


def list_sessions() -> list[dict]:
    """List all saved sessions (newest first)."""
    sessions = []
    for filepath in sorted(SESSIONS_DIR.glob("session_*.json"), reverse=True):
        try:
            with open(filepath) as f:
                data = json.load(f)
                sessions.append({
                    "id": data["id"],
                    "timestamp": data["timestamp"],
                    "question": data["question"][:100] + ("..." if len(data["question"]) > 100 else ""),
                    "consensus_score": data["consensus"]["score"],
                    "consensus_level": data["consensus"]["level"],
                    "models": data["models"],
                    "filepath": str(filepath)
                })
        except Exception:
            continue
    return sessions


def load_session(session_id: str) -> dict | None:
    """Load a specific session by ID."""
    filepath = SESSIONS_DIR / f"session_{session_id}.json"
    if filepath.exists():
        with open(filepath) as f:
            return json.load(f)
    return None


def export_session_to_markdown(session_id: str) -> str:
    """Export a session to Markdown format."""
    session = load_session(session_id)
    if not session:
        return ""

    md = f"""# Council Session: {session['id']}

**Date:** {session['timestamp']}

**Question:** {session['question']}

**Models:** {', '.join(session['models'])}

**Consensus:** {session['consensus']['level'].upper()} ({session['consensus']['score']}/100)
{session['consensus']['description']}

---

## Individual Responses

"""
    for model, resp in session['responses'].items():
        md += f"### {model}\n*Generated in {resp['elapsed_seconds']:.1f}s*\n\n{resp['content']}\n\n---\n\n"

    md += "## Peer Reviews\n\n"
    for reviewer, review in session['reviews'].items():
        md += f"### Review by {reviewer}\n\n{review['analysis']}\n\n---\n\n"

    md += f"""## Chairman's Synthesis

{session['synthesis']}

---

*Generated by LLM Council*
"""
    return md


# --- State ---

verbose_mode: bool = False


# --- Formatting Helpers ---

def format_response_tabs(responses: dict) -> str:
    """Format individual responses as markdown with tabs simulation."""
    if not responses:
        return "*No responses yet*"

    md = ""
    for model, response in responses.items():
        display_name = get_display_name(model)
        elapsed = response.elapsed_seconds
        md += f"### {display_name}\n"
        md += f"*Generated in {elapsed:.1f}s*\n\n"
        md += f"{response.content}\n\n"
        md += "---\n\n"

    return md


def format_responses_side_by_side(partial_responses: dict, complete_responses: dict, models: list) -> str:
    """Format responses in a side-by-side grid layout with live streaming text."""
    import html as html_module

    if not partial_responses and not complete_responses:
        return "<em>Waiting for responses...</em>"

    if not models:
        return "<em>No models selected</em>"

    # Build HTML grid for side-by-side display
    num_models = len(models)
    cols = min(num_models, 3)  # Max 3 columns

    html = f'<div style="display: grid; grid-template-columns: repeat({cols}, 1fr); gap: 16px;">'

    for model in models:
        display_name = html_module.escape(get_display_name(model))
        is_complete = model in complete_responses

        # Get content
        if is_complete:
            content = complete_responses[model].content
            elapsed = complete_responses[model].elapsed_seconds
            status = f"‚úÖ {elapsed:.1f}s"
            border_color = "#4CAF50"
        else:
            content = partial_responses.get(model, "")
            status = "‚è≥ generating..."
            border_color = "#FF9800"

        # Escape HTML in content and truncate if too long
        content = html_module.escape(content) if content else ""
        display_content = content if len(content) < 2000 else "..." + content[-1800:]

        html += f'''
        <div style="border: 2px solid {border_color}; border-radius: 8px; padding: 12px; background: #fafafa; min-height: 200px; max-height: 400px; overflow-y: auto;">
            <div style="font-weight: bold; color: #333; border-bottom: 1px solid #ddd; padding-bottom: 8px; margin-bottom: 8px;">
                {display_name} <span style="font-weight: normal; color: {'#4CAF50' if is_complete else '#FF9800'};">{status}</span>
            </div>
            <div style="font-size: 0.9em; white-space: pre-wrap; word-wrap: break-word;">{display_content or "<em>Starting...</em>"}</div>
        </div>
        '''

    html += '</div>'
    return html


def format_reviews_side_by_side(partial_reviews: dict, complete_reviews: dict, models: list) -> str:
    """Format reviews in a side-by-side grid layout with live streaming text."""
    import html as html_module

    if not partial_reviews and not complete_reviews:
        return "<em>Waiting for reviews...</em>"

    if not models:
        return "<em>No reviewers</em>"

    num_models = len(models)
    cols = min(num_models, 3)

    html = f'<div style="display: grid; grid-template-columns: repeat({cols}, 1fr); gap: 16px;">'

    for model in models:
        display_name = html_module.escape(get_display_name(model))
        is_complete = model in complete_reviews

        if is_complete:
            content = complete_reviews[model].analysis
            status = "‚úÖ complete"
            border_color = "#4CAF50"
        else:
            content = partial_reviews.get(model, "")
            status = "‚è≥ reviewing..."
            border_color = "#FF9800"

        # Escape HTML in content and truncate if too long
        content = html_module.escape(content) if content else ""
        display_content = content if len(content) < 1500 else "..." + content[-1300:]

        html += f'''
        <div style="border: 2px solid {border_color}; border-radius: 8px; padding: 12px; background: #fff8f0; min-height: 150px; max-height: 350px; overflow-y: auto;">
            <div style="font-weight: bold; color: #333; border-bottom: 1px solid #ddd; padding-bottom: 8px; margin-bottom: 8px;">
                {display_name} <span style="font-weight: normal; color: {'#4CAF50' if is_complete else '#FF9800'};">{status}</span>
            </div>
            <div style="font-size: 0.9em; white-space: pre-wrap; word-wrap: break-word;">{display_content or "<em>Starting review...</em>"}</div>
        </div>
        '''

    html += '</div>'
    return html


def format_reviews(reviews: dict, responses: dict) -> str:
    """Format peer reviews as markdown."""
    if not reviews:
        return "*No reviews yet*"

    md = ""
    for reviewer, review in reviews.items():
        reviewer_name = get_display_name(reviewer)
        md += f"### Review by {reviewer_name}\n\n"

        # Show which responses they reviewed
        reviewed = [get_display_name(m) for m in responses.keys() if m != reviewer]
        md += f"*Reviewed: {', '.join(reviewed)}*\n\n"

        md += f"{review.analysis}\n\n"
        md += "---\n\n"

    return md


def format_synthesis(synthesis: str) -> str:
    """Format chairman synthesis for Tab 2 (full details)."""
    if not synthesis:
        return "*Awaiting chairman synthesis...*"

    chairman_name = get_display_name(CHAIRMAN_MODEL)
    return f"## {chairman_name}\n\n{synthesis}"


def format_synthesis_brief(synthesis: str) -> str:
    """Format chairman synthesis for Tab 1 (only the main answer, no meta-analysis)."""
    if not synthesis:
        return "*Awaiting chairman synthesis...*"

    # Extract just the SYNTHESIS section, skip KEY CONTRIBUTORS, CONSENSUS, DISAGREEMENT
    lines = synthesis.split('\n')
    brief_lines = []
    in_synthesis = False
    skip_sections = ['KEY CONTRIBUTORS', 'AREAS OF CONSENSUS', 'AREAS OF DISAGREEMENT',
                     '2.', '3.', '4.']

    for line in lines:
        # Check if we hit a section to skip
        should_skip = any(skip in line.upper() for skip in ['KEY CONTRIBUTOR', 'AREAS OF CONSENSUS', 'AREAS OF DISAGREEMENT'])
        if should_skip or (line.strip().startswith(('2.', '3.', '4.')) and any(x in line.upper() for x in ['KEY', 'AREA', 'CONSENSUS', 'DISAGREEMENT'])):
            break
        brief_lines.append(line)

    return '\n'.join(brief_lines).strip()


def format_consensus_meter(consensus: ConsensusScore | None) -> str:
    """Generate a visual consensus meter with score and interpretation."""
    if consensus is None:
        return "*Calculating consensus...*"

    if consensus.level == "unknown":
        return f"*{consensus.description}*"

    # Color based on level
    colors = {
        "high": ("#4CAF50", "#e8f5e9"),  # Green
        "medium": ("#FF9800", "#fff3e0"),  # Orange
        "low": ("#f44336", "#ffebee"),  # Red
    }
    fg_color, bg_color = colors.get(consensus.level, ("#9e9e9e", "#f5f5f5"))

    # Build the meter bar
    fill_pct = consensus.score
    level_emoji = {"high": "üü¢", "medium": "üü°", "low": "üî¥"}.get(consensus.level, "‚ö™")

    html = f'''
<div style="background: {bg_color}; border: 2px solid {fg_color}; border-radius: 12px; padding: 16px; margin: 8px 0;">
    <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 8px;">
        <span style="font-weight: bold; color: #333;">Consensus Score</span>
        <span style="font-size: 1.2em; font-weight: bold; color: {fg_color};">{level_emoji} {consensus.score:.0f}/100</span>
    </div>

    <div style="background: #e0e0e0; border-radius: 8px; height: 12px; overflow: hidden; margin-bottom: 12px;">
        <div style="background: {fg_color}; height: 100%; width: {fill_pct}%; transition: width 0.3s ease;"></div>
    </div>

    <div style="font-size: 0.95em; color: #555;">
        <strong>{consensus.level.upper()} CONSENSUS:</strong> {consensus.description}
    </div>
'''

    # Show disagreement areas if any
    if consensus.disagreement_areas:
        html += '''
    <div style="margin-top: 12px; padding-top: 12px; border-top: 1px solid #ddd;">
        <strong style="color: #333;">Key Areas of Disagreement:</strong>
        <ul style="margin: 8px 0 0 0; padding-left: 20px; color: #666;">
'''
        for area in consensus.disagreement_areas:
            html += f'            <li>{area}</li>\n'
        html += '        </ul>\n    </div>\n'

    html += '</div>'
    return html


def calculate_agreement_summary(reviews: dict, responses: dict = None) -> str:
    """Generate agreement/disagreement summary from reviews using consensus calculation."""
    if not reviews or len(reviews) < 2:
        return "*Need at least 2 reviews to calculate consensus*"

    # Calculate actual consensus if we have responses
    if responses:
        consensus = calculate_consensus(reviews, responses)
        return format_consensus_meter(consensus)

    # Fallback if no responses provided
    num_reviewers = len(reviews)
    return f"**{num_reviewers} council members** participated in peer review."


# --- Main Council Runner ---

async def run_council_streaming(question: str, selected_models: list[str], update_queue):
    """
    Run the council with live token streaming updates via queue.
    Shows text as models generate it in side-by-side boxes.
    """
    if not question.strip():
        await update_queue.put((
            "Please enter a question.",
            "", "", "",
            "No responses yet",
            "No reviews yet",
            "Awaiting synthesis...",
            "",
            ""  # consensus_display_tab1
        ))
        return

    # Map display names back to model IDs
    name_to_id = {v: k for k, v in MODEL_DISPLAY_NAMES.items()}
    active_models = [name_to_id[name] for name in selected_models if name in name_to_id]

    if not active_models:
        await update_queue.put((
            "No council members selected! Please select at least one model.",
            "", "", "",
            "No models selected",
            "No reviews possible",
            "Cannot synthesize without responses",
            "",
            ""  # consensus_display_tab1
        ))
        return

    model_names = [get_display_name(m) for m in active_models]
    complete_responses = {}
    complete_reviews = {}

    # Throttle updates to avoid overwhelming the UI
    import time as time_module
    last_update = [0]
    update_interval = 0.3  # Update UI every 300ms max

    # --- Stage 1: Initial Responses with live streaming ---
    await update_queue.put((
        f"**Stage 1/3:** Generating responses ({len(active_models)} models)...",
        "",
        "",
        "",
        format_responses_side_by_side({m: "" for m in active_models}, {}, active_models),
        "### ‚è∏Ô∏è Waiting for Stage 1...\n\n*Peer reviews will begin after all responses are complete.*",
        "### ‚è∏Ô∏è Waiting for responses and reviews...\n\n*Chairman will synthesize after peer reviews.*",
        "",
        ""  # consensus_display_tab1
    ))

    # Token callback - update UI with partial responses
    async def on_token_stage1(partial_responses, done_responses):
        now = time_module.time()
        if now - last_update[0] < update_interval:
            return  # Throttle updates
        last_update[0] = now

        completed = len(done_responses)
        total = len(active_models)
        status = f"**Stage 1/3:** Generating responses ({completed}/{total} complete)..."

        await update_queue.put((
            status,
            "",
            "",
            "",
            format_responses_side_by_side(partial_responses, done_responses, active_models),
            "### ‚è∏Ô∏è Waiting for Stage 1...\n\n*Peer reviews will begin after all responses are complete.*",
            "### ‚è∏Ô∏è Waiting for responses and reviews...",
            "",
            ""  # consensus_display_tab1
        ))

    # Complete callback
    async def on_complete_stage1(model, response, all_done):
        complete_responses[model] = response
        completed = len(all_done)
        total = len(active_models)

        await update_queue.put((
            f"**Stage 1/3:** Generating responses ({completed}/{total} complete)...",
            "",
            "",
            "",
            format_responses_side_by_side({m: complete_responses.get(m, type('', (), {'content': ''})()).content if m in complete_responses else "" for m in active_models}, complete_responses, active_models),
            "### ‚è∏Ô∏è Waiting for Stage 1...",
            "### ‚è∏Ô∏è Waiting for responses and reviews...",
            "",
            ""  # consensus_display_tab1
        ))

    # Run Stage 1 with live streaming
    responses = await stream_initial_responses_live(question, active_models, on_token_stage1, on_complete_stage1)
    complete_responses = responses

    # Format timing summary
    timing_summary = "**Response Times:**\n"
    for model in active_models:
        if model in responses:
            timing_summary += f"- {get_display_name(model)}: ‚úÖ {responses[model].elapsed_seconds:.1f}s\n"

    # --- Stage 2: Peer Reviews with live streaming ---
    reviewer_models = list(responses.keys())

    await update_queue.put((
        f"**Stage 2/3:** Peer review in progress ({len(responses)} reviewers)...",
        "",
        timing_summary,
        "",
        format_responses_side_by_side({}, responses, active_models),
        format_reviews_side_by_side({m: "" for m in reviewer_models}, {}, reviewer_models),
        "### ‚è∏Ô∏è Waiting for peer reviews...\n\n*Chairman synthesis will begin after all reviews are done.*",
        "",
        ""  # consensus_display_tab1
    ))

    # Token callback for Stage 2
    async def on_token_stage2(partial_reviews, done_reviews):
        now = time_module.time()
        if now - last_update[0] < update_interval:
            return
        last_update[0] = now

        completed = len(done_reviews)
        total = len(reviewer_models)
        consensus_html = calculate_agreement_summary(done_reviews, responses) if len(done_reviews) >= 2 else ""

        await update_queue.put((
            f"**Stage 2/3:** Peer review ({completed}/{total} complete)...",
            "",
            timing_summary,
            "",
            format_responses_side_by_side({}, responses, active_models),
            format_reviews_side_by_side(partial_reviews, done_reviews, reviewer_models),
            "### ‚è∏Ô∏è Waiting for peer reviews...",
            consensus_html,
            consensus_html  # consensus_display_tab1
        ))

    # Complete callback for Stage 2
    async def on_complete_stage2(reviewer, review, all_done):
        complete_reviews[reviewer] = review
        completed = len(all_done)
        total = len(reviewer_models)
        consensus_html = calculate_agreement_summary(all_done, responses) if len(all_done) >= 2 else ""

        await update_queue.put((
            f"**Stage 2/3:** Peer review ({completed}/{total} complete)...",
            "",
            timing_summary,
            "",
            format_responses_side_by_side({}, responses, active_models),
            format_reviews_side_by_side({m: complete_reviews.get(m, type('', (), {'analysis': ''})()).analysis if m in complete_reviews else "" for m in reviewer_models}, complete_reviews, reviewer_models),
            "### ‚è∏Ô∏è Waiting for peer reviews...",
            consensus_html,
            consensus_html  # consensus_display_tab1
        ))

    # Run Stage 2 with live streaming
    reviews = await stream_peer_reviews_live(question, responses, on_token_stage2, on_complete_stage2)
    complete_reviews = reviews

    # --- Stage 3: Chairman Synthesis ---
    # Calculate consensus from reviews
    consensus = calculate_consensus(reviews, responses)
    consensus_display = format_consensus_meter(consensus)

    await update_queue.put((
        "**Stage 3/3:** Chairman synthesizing final answer...\n\n*Claude Opus 4.5 is analyzing all responses and reviews...*",
        "",
        timing_summary,
        "",
        format_responses_side_by_side({}, responses, active_models),
        format_reviews_side_by_side({}, reviews, reviewer_models),
        "### üîÑ Chairman is synthesizing...\n\n*Analyzing all responses and peer reviews to create the best answer.*",
        consensus_display,
        consensus_display  # consensus_display_tab1
    ))

    # Pass consensus to chairman so it can emphasize disagreements when needed
    synthesis = await get_chairman_synthesis(question, responses, reviews, consensus)

    # Save session to JSON
    session_data = save_session(
        question=question,
        responses=responses,
        reviews=reviews,
        synthesis=synthesis,
        consensus=consensus,
        models=active_models
    )

    # Final results
    final_status = f"**Council session complete!**\n\nActive members: {', '.join(model_names)}"

    await update_queue.put((
        final_status,
        format_synthesis_brief(synthesis),  # Tab 1: brief synthesis only
        timing_summary,
        "",
        format_responses_side_by_side({}, responses, active_models),
        format_reviews_side_by_side({}, reviews, reviewer_models),
        format_synthesis(synthesis),  # Tab 2: full synthesis with all sections
        consensus_display,  # Show consensus meter with score
        consensus_display  # consensus_display_tab1
    ))


def run_council_generator(question: str, selected_models: list[str], progress=gr.Progress()):
    """
    Generator wrapper for streaming updates to all tabs.
    Yields updates as each model completes.
    """
    import queue
    import threading

    # Thread-safe queue for updates
    result_queue = queue.Queue()

    def run_async():
        async def run_with_queue():
            # Create async queue
            async_queue = asyncio.Queue()

            # Task to transfer from async queue to sync queue
            async def transfer():
                while True:
                    item = await async_queue.get()
                    if item is None:
                        break
                    result_queue.put(("update", item))
                result_queue.put(("done", None))

            # Run council and transfer concurrently
            transfer_task = asyncio.create_task(transfer())
            await run_council_streaming(question, selected_models, async_queue)
            await async_queue.put(None)  # Signal completion
            await transfer_task

        asyncio.run(run_with_queue())

    # Start async runner in thread
    thread = threading.Thread(target=run_async)
    thread.start()

    # Yield updates as they come
    while True:
        try:
            msg_type, data = result_queue.get(timeout=600)  # 10 min timeout
            if msg_type == "done":
                break
            yield data
        except queue.Empty:
            break

    thread.join()


# --- Settings Callbacks ---

def toggle_verbose(value: bool) -> str:
    """Toggle verbose mode."""
    global verbose_mode
    verbose_mode = value
    return f"Verbose mode: {'ON' if value else 'OFF'}"


# --- Build the Interface ---

def create_app():
    """Create the Gradio application."""

    with gr.Blocks(title="LLM Council") as app:

        gr.Markdown(
            """
            # LLM Council
            ### Multi-model deliberation with peer review and chairman synthesis
            """,
            elem_classes="council-header"
        )

        with gr.Tabs():

            # === TAB 1: Ask the Council ===
            with gr.Tab("Ask the Council"):
                with gr.Row():
                    with gr.Column(scale=2):
                        question_input = gr.Textbox(
                            label="Your Question",
                            placeholder="Enter a question for the council to deliberate...",
                            lines=3,
                            elem_classes="question-input"
                        )

                        # Model selection - vertical checkboxes with clear labels
                        gr.Markdown("#### Select Council Members")
                        model_checkboxes = []
                        for model_id in AVAILABLE_MODELS:
                            display_name = get_display_name(model_id)
                            is_default = model_id in DEFAULT_ENABLED_MODELS
                            cb = gr.Checkbox(
                                label=display_name,
                                value=is_default,
                                elem_classes="model-checkbox"
                            )
                            model_checkboxes.append(cb)

                        submit_btn = gr.Button("Submit to Council", variant="primary", size="lg")

                    with gr.Column(scale=1):
                        status_display = gr.Markdown(
                            value="Ready to convene the council.",
                            elem_classes="stage-indicator"
                        )
                        timing_display = gr.Markdown(value="")

                gr.Markdown("### Chairman's Final Answer", elem_classes="final-answer-header")
                final_answer = gr.Markdown(
                    value="*Submit a question to see the council's synthesized answer.*",
                    elem_classes="final-answer"
                )

                # Consensus meter for Tab 1
                gr.Markdown("### Council Consensus")
                consensus_display_tab1 = gr.HTML(value="<em>Submit a question to see consensus analysis</em>")

                # Hidden outputs for other tabs (updated together)
                hidden_spacer = gr.Markdown(value="", visible=False)

            # === TAB 2: Council Deliberation ===
            with gr.Tab("Council Deliberation"):
                gr.Markdown("### Detailed view of the deliberation process")

                with gr.Accordion("Stage 1: Individual Responses", open=True, elem_classes="stage-1"):
                    responses_display = gr.HTML(value="<em>No responses yet</em>")

                with gr.Accordion("Stage 2: Peer Reviews", open=True, elem_classes="stage-2"):
                    reviews_display = gr.HTML(value="<em>No reviews yet</em>")
                    gr.Markdown("#### Consensus Meter")
                    agreement_display = gr.HTML(value="<em>Submit a question to see consensus analysis</em>")

                with gr.Accordion("Stage 3: Chairman's Analysis", open=True, elem_classes="stage-3"):
                    synthesis_display = gr.Markdown(value="*Awaiting synthesis...*", elem_classes="stage-3-content")

            # === TAB 3: Settings ===
            with gr.Tab("Settings"):
                gr.Markdown("### Council Configuration")

                with gr.Row():
                    with gr.Column():
                        gr.Markdown("#### Chairman")
                        gr.Markdown(f"**{get_display_name(CHAIRMAN_MODEL)}**")
                        gr.Markdown("*The chairman synthesizes the final answer from all council deliberations.*")

                    with gr.Column():
                        gr.Markdown("#### Display Options")

                        verbose_toggle = gr.Checkbox(
                            label="Verbose Mode",
                            value=False,
                            info="Show additional detail in responses"
                        )
                        verbose_status = gr.Markdown(value="Verbose mode: OFF")

                gr.Markdown("---")
                gr.Markdown(
                    """
                    #### Model Information
                    | Model | Size | Description |
                    |-------|------|-------------|
                    | Llama 3.3 70B | ~40GB | Meta's flagship model |
                    | Qwen 2.5 32B | ~20GB | Alibaba's multilingual model |
                    | Gemma 2 27B | ~17GB | Google's efficient model |
                    | DeepSeek R1 32B | ~20GB | Reasoning specialist |
                    | Mistral 7B | ~4GB | Fast European model |
                    | Claude Opus 4.5 | API | Chairman (Anthropic) |
                    """
                )

                verbose_toggle.change(
                    fn=toggle_verbose,
                    inputs=[verbose_toggle],
                    outputs=[verbose_status]
                )

            # === TAB 4: History ===
            with gr.Tab("History"):
                gr.Markdown("### Past Council Sessions")
                gr.Markdown("Browse and export previous deliberations.")

                with gr.Row():
                    refresh_btn = gr.Button("üîÑ Refresh", size="sm")
                    export_md_btn = gr.Button("üìÑ Export to Markdown", size="sm")

                session_dropdown = gr.Dropdown(
                    label="Select Session",
                    choices=[],
                    interactive=True,
                    elem_classes="session-dropdown"
                )

                with gr.Row():
                    session_info = gr.HTML(value="<em>Select a session to view details</em>")

                with gr.Accordion("Session Details", open=True):
                    session_question = gr.Markdown(value="")
                    session_consensus = gr.HTML(value="")
                    session_synthesis = gr.Markdown(value="")

                markdown_output = gr.Textbox(
                    label="Markdown Export (copy to clipboard)",
                    lines=10,
                    visible=False
                )

                # History tab callbacks
                def refresh_sessions():
                    sessions = list_sessions()
                    if not sessions:
                        return gr.update(choices=[], value=None), "<em>No sessions found</em>"
                    choices = [
                        (f"{s['timestamp'][:10]} | {s['consensus_level'].upper()} ({s['consensus_score']:.0f}) | {s['question'][:50]}...", s['id'])
                        for s in sessions
                    ]
                    return gr.update(choices=choices, value=None), f"<em>Found {len(sessions)} sessions</em>"

                def load_session_details(session_id):
                    if not session_id:
                        return "", "", ""
                    session = load_session(session_id)
                    if not session:
                        return "Session not found", "", ""

                    question_md = f"**Question:** {session['question']}\n\n**Models:** {', '.join(session['models'])}\n\n**Date:** {session['timestamp']}"

                    consensus = session['consensus']
                    colors = {"high": ("#4CAF50", "#e8f5e9"), "medium": ("#FF9800", "#fff3e0"), "low": ("#f44336", "#ffebee")}
                    fg, bg = colors.get(consensus['level'], ("#9e9e9e", "#f5f5f5"))
                    emoji = {"high": "üü¢", "medium": "üü°", "low": "üî¥"}.get(consensus['level'], "‚ö™")

                    consensus_html = f'''
                    <div style="background: {bg}; border: 2px solid {fg}; border-radius: 8px; padding: 12px; margin: 8px 0;">
                        <strong>{emoji} {consensus['level'].upper()} CONSENSUS</strong> ({consensus['score']}/100)<br/>
                        {consensus['description']}
                    </div>
                    '''

                    synthesis_md = f"### Chairman's Synthesis\n\n{session['synthesis']}"

                    return question_md, consensus_html, synthesis_md

                def export_markdown(session_id):
                    if not session_id:
                        return gr.update(visible=False, value="")
                    md = export_session_to_markdown(session_id)
                    return gr.update(visible=True, value=md)

                refresh_btn.click(
                    fn=refresh_sessions,
                    outputs=[session_dropdown, session_info]
                )

                session_dropdown.change(
                    fn=load_session_details,
                    inputs=[session_dropdown],
                    outputs=[session_question, session_consensus, session_synthesis]
                )

                export_md_btn.click(
                    fn=export_markdown,
                    inputs=[session_dropdown],
                    outputs=[markdown_output]
                )

                # Load sessions on startup
                app.load(
                    fn=refresh_sessions,
                    outputs=[session_dropdown, session_info]
                )

        # === Main Submit Action ===
        # Wrapper to collect checkbox values and call generator
        def collect_and_run(question, *checkbox_values):
            # Build list of selected model display names
            selected = []
            for i, is_checked in enumerate(checkbox_values):
                if is_checked:
                    model_id = AVAILABLE_MODELS[i]
                    selected.append(get_display_name(model_id))
            # Return generator results
            yield from run_council_generator(question, selected)

        submit_btn.click(
            fn=collect_and_run,
            inputs=[question_input] + model_checkboxes,
            outputs=[
                status_display,
                final_answer,
                timing_display,
                hidden_spacer,
                responses_display,
                reviews_display,
                synthesis_display,
                agreement_display,
                consensus_display_tab1,
            ],
        )

    return app


# --- Entry Point ---

if __name__ == "__main__":
    app = create_app()
    app.launch(
        server_name="0.0.0.0",
        server_port=7861,
        share=False,
        show_error=True,
        theme=gr.themes.Soft(),
        css="""
        .council-header { text-align: center; margin-bottom: 20px; }
        .stage-indicator { font-size: 1.1em; padding: 10px; border-radius: 8px; background: #f0f0f0; }

        /* Model checkboxes styling */
        .model-checkbox {
            padding: 8px 12px;
            margin: 4px 0;
            border: 1px solid #e0e0e0;
            border-radius: 6px;
            background: #fafafa;
        }
        .model-checkbox:hover {
            background: #f0f0f0;
        }

        /* Stage 1 - Blue theme */
        .stage-1 {
            border-left: 4px solid #2196F3 !important;
            background: linear-gradient(to right, #e3f2fd, transparent) !important;
        }
        .stage-1-content {
            border-left: 2px solid #2196F3;
            padding-left: 12px;
            margin-left: 8px;
        }

        /* Stage 2 - Orange theme */
        .stage-2 {
            border-left: 4px solid #FF9800 !important;
            background: linear-gradient(to right, #fff3e0, transparent) !important;
        }
        .stage-2-content {
            border-left: 2px solid #FF9800;
            padding-left: 12px;
            margin-left: 8px;
        }

        /* Stage 3 - Green theme */
        .stage-3 {
            border-left: 4px solid #4CAF50 !important;
            background: linear-gradient(to right, #e8f5e9, transparent) !important;
        }
        .stage-3-content {
            border-left: 2px solid #4CAF50;
            padding-left: 12px;
            margin-left: 8px;
        }

        /* Chairman's Final Answer - Purple/Gold theme */
        .final-answer-header {
            color: #6A1B9A;
        }
        .final-answer {
            border: 2px solid #9C27B0;
            border-radius: 12px;
            padding: 20px;
            background: linear-gradient(135deg, #f3e5f5 0%, #fff8e1 100%);
            box-shadow: 0 4px 6px rgba(156, 39, 176, 0.1);
        }

        /* Question Input - Purple/Gold theme */
        .question-input {
            border: 2px solid #9C27B0 !important;
            border-radius: 12px !important;
            background: linear-gradient(135deg, #f3e5f5 0%, #fff8e1 100%) !important;
            box-shadow: 0 4px 6px rgba(156, 39, 176, 0.1) !important;
        }
        .question-input textarea {
            background: transparent !important;
        }
        .question-input label {
            color: #6A1B9A !important;
            font-weight: bold !important;
        }
        """,
    )
